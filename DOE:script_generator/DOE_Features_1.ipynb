{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOE: Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: This notebook just generates potential experiment groups of different sizes (k) - final experiment groups were chosen manually (see excel sheet)\n",
    "\n",
    "#### Description:\n",
    "\n",
    "1) Read in language vectors and their dataset abbreviation\n",
    "\n",
    "2) Remove investigated feature from the dataframe and find k closest vectors for feature\n",
    "\n",
    "3) Create overview over all features for k\n",
    "\n",
    "4) Generate for each k a csv-file -> experiment groups are chosen manually in excel\n",
    "\n",
    "5) Create scripts per feature from manual choice (csv-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.neighbors as nei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Read in language vectors and corresponding Wikiann data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len before removing lan < 100T lines: 38\n",
      "len lan < 100T lines: 7\n",
      "len after removing lan < 100T lines: 31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>language</th>\n",
       "      <th>Wikiann</th>\n",
       "      <th>script</th>\n",
       "      <th>Member in European Sprachbund</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>Latin</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Grm</td>\n",
       "      <td>de</td>\n",
       "      <td>Latin</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Dut</td>\n",
       "      <td>nl</td>\n",
       "      <td>Latin</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Eng</td>\n",
       "      <td>en</td>\n",
       "      <td>Latin</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Grk</td>\n",
       "      <td>el</td>\n",
       "      <td>Greek</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index language Wikiann script Member in European Sprachbund    1  \\\n",
       "0        0    0.0       Fr      fr  Latin                          True  1.0   \n",
       "1        1    1.0      Grm      de  Latin                          True  1.0   \n",
       "2        2    2.0      Dut      nl  Latin                          True  1.0   \n",
       "3        3    3.0      Eng      en  Latin                          True  1.0   \n",
       "4        4    4.0      Grk      el  Greek                          True  1.0   \n",
       "\n",
       "     2    3    4    5    6    7    8    9   10   11   12  \n",
       "0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "1  1.0  1.0  1.0  1.0  1.0  1.0  1.0  0.0  1.0  1.0  1.0  \n",
       "2  1.0  1.0  1.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  \n",
       "3  1.0  1.0  1.0  1.0  1.0  0.0  1.0  1.0  1.0  1.0  0.0  \n",
       "4  1.0  1.0  1.0  1.0  1.0  1.0  0.0  1.0  1.0  0.0  1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lan_EuropeanSprachbund_raw = pd.read_csv('overview_EuropeanSprachbund.csv', sep=';')\n",
    "\n",
    "#remove empty lines at the end of the dataframe\n",
    "lan_EuropeanSprachbund_raw.drop([38,39], inplace=True)\n",
    "\n",
    "# remove languages with dataset < 100T lines\n",
    "print('len before removing lan < 100T lines: ' + str(len(lan_EuropeanSprachbund_raw)))\n",
    "\n",
    "languages_with_less_100Tlines = [\"Ice\",\"Ir\",\"Gae\",\"Kom\",\"Mlt\",\"Srd\",\"Udm\"]\n",
    "print('len lan < 100T lines: ' + str(len(languages_with_less_100Tlines)))\n",
    "indices = [i for i in range(0,len(lan_EuropeanSprachbund_raw.index)) if lan_EuropeanSprachbund_raw['language'][i] not in languages_with_less_100Tlines]\n",
    "lan_EuropeanSprachbund = lan_EuropeanSprachbund_raw.iloc[indices].reset_index()\n",
    "\n",
    "print('len after removing lan < 100T lines: ' + str(len(lan_EuropeanSprachbund)))\n",
    "\n",
    "lan_EuropeanSprachbund.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Remove investigated feature from dataframe and find k closest vectors for feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_feature_from_df(df_raw, featurenumber):\n",
    "    \n",
    "    df = df_raw[df_raw.columns[6:18]]\n",
    "    df = df.drop([str(featurenumber)],axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average distance of several distances\n",
    "def average( a , n ): \n",
    "  \n",
    "    # Find sum of array element \n",
    "    sum = 0\n",
    "    for i in range(n): \n",
    "        sum += a[i] \n",
    "      \n",
    "    return sum/n; \n",
    "\n",
    "def average_dist(dis):\n",
    "    return average(dis[0], len(dis[0]))\n",
    "\n",
    "\n",
    "# calculates closest k vectors for a feature\n",
    "def get_k_closest_languages_per_feature(df_raw, featurenumber, k):\n",
    "    \n",
    "    df = remove_feature_from_df(df_raw, featurenumber)\n",
    "\n",
    "    distanceMetric = nei.DistanceMetric.get_metric('euclidean')\n",
    "    matrix = df.to_numpy()\n",
    "    tree = nei.KDTree(matrix, metric=distanceMetric)\n",
    "\n",
    "    # calculate for each vector average distance to k closest vectors\n",
    "    avg_distances_per_lan = {}\n",
    "    \n",
    "    for index in range(0,(len(matrix))):\n",
    "        dist, ind = tree.query(matrix[index:(index+1)], k)\n",
    "        avg_distances_per_lan[index] = average_dist(dist)\n",
    "    \n",
    "    # language with k closest neighbors\n",
    "    \n",
    "    lan_with_closest_k_neighbors = pd.DataFrame.from_dict(avg_distances_per_lan, orient='index', columns=['average_dist']).sort_values(by=['average_dist']).index[0]\n",
    "        #print(lan_with_closest_k_neighbors)\n",
    "    vec_lan_with_k_closest_neighbors = matrix[lan_with_closest_k_neighbors:(lan_with_closest_k_neighbors+1)]\n",
    "        #print(df.iloc[lan_with_closest_k_neighbors])\n",
    "        #print(vec_lan_with_k_closest_neighbors)\n",
    "    \n",
    "    dist, ind = tree.query(vec_lan_with_k_closest_neighbors, k)\n",
    "    \n",
    "        #print(ind)\n",
    "        #print(dist)\n",
    "    \n",
    "     # k closest languages\n",
    "    list = []\n",
    "    for i in ind[0]:\n",
    "        list.append(df_raw['language'].iloc[i])\n",
    "        \n",
    "    #print(list)\n",
    "    \n",
    "    \n",
    "    # filter df for closest vectors    \n",
    "    Filter_df_raw  = df_raw[df_raw['language'].isin(list)]\n",
    "        #print(Filter_df_raw)\n",
    "    \n",
    "   \n",
    "    # create overview for feature with needed attributes\n",
    "    df_feat = pd.DataFrame(columns=['feature','average_dist_over_feat','language','index_org','Hasfeature','dist'])\n",
    "    list_of_Series = []\n",
    "    \n",
    "    for lan in Filter_df_raw.index:\n",
    "        \n",
    "        lang = Filter_df_raw['language'][lan]\n",
    "        \n",
    "        Hasfeature = False\n",
    "        if Filter_df_raw[str(featurenumber)][lan] == 1:\n",
    "            Hasfeature = True\n",
    "            \n",
    "        #distance of language-pair\n",
    "        for index, item in enumerate(ind[0]):\n",
    "            if item == lan:\n",
    "                distance_lan = dist[0][index]\n",
    "                \n",
    "        list_of_Series += [pd.Series([featurenumber, average_dist(dist), lang, lan, Hasfeature, distance_lan], index=df_feat.columns)]\n",
    "\n",
    "    df_feat_added = df_feat.append(list_of_Series, ignore_index=True)\n",
    "    \n",
    "    return Filter_df_raw, df_feat_added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Create overview over all features for k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe and csv for all features\n",
    "def overview_groups_all_features(df_raw, k):\n",
    "\n",
    "    frames_per_feature = []\n",
    "\n",
    "    for feat in range(1,(len(df_raw.columns[6:18])+1)):\n",
    "        _, df_per_feat = get_k_closest_languages_per_feature(df_raw, feat, k)\n",
    "        frames_per_feature += [df_per_feat]\n",
    "\n",
    "    doe_features = pd.concat(frames_per_feature)\n",
    "    \n",
    "    return doe_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Generate for each k a csv-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(df_raw, k):\n",
    "    \n",
    "    df = overview_groups_all_features(df_raw, k)\n",
    "    \n",
    "    df.to_csv('DOE_typFeatures_'+ str(k)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution : ___main___ 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 10\n",
    "to_csv(lan_EuropeanSprachbund, k1)\n",
    "\n",
    "k2 = 15\n",
    "to_csv(lan_EuropeanSprachbund, k2)\n",
    "\n",
    "k3 = 20\n",
    "to_csv(lan_EuropeanSprachbund, k3)\n",
    "\n",
    "k4 = 25\n",
    "to_csv(lan_EuropeanSprachbund, k4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Generate experiment script files: Not done yet (phze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from language similarity\n",
    "def generate_string_dataset(wikiann_abre):\n",
    "    return 'languages/' + wikiann_abre + '/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from language similarity\n",
    "\n",
    "def generate_script_file(df_raw, d):\n",
    "    \n",
    "    df = dist_filter(df_raw, d)\n",
    "    number_of_experiments = df['DIST'].value_counts().sort_index()[d]\n",
    "    \n",
    "    file = open('run_transformers.sh','w')\n",
    "    \n",
    "    file.write('# Experiments for distance: ' + str(d) + '\\n')\n",
    "    file.write('\\n')\n",
    "    file.write('# Experiment design: \\n')\n",
    "    file.write('# (Finetuned language, Evaluated language) - Split(80/20)\\n')\n",
    "    file.write('# Number of language pairs: ' + str(number_of_experiments) + '\\n')\n",
    "    file.write('\\n')\n",
    "    file.write('# @author: jopo, phze \\n')\n",
    "    file.write('# @date: ' + str(datetime.datetime.now()) + '\\n')\n",
    "    file.write('\\n')\n",
    "    file.write('\\n')\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        file.write ('# ' + str(index) + ' - Language pair: ' + '(' + row['LAN_1'] + ',' + row['LAN_2'] + ')\\n')\n",
    "        \n",
    "        #creates test-set '4350' 20% -> dataset is of '17003' sentences = 80% training set\n",
    "        file.write('python3 preprocessing/take_sentences.py \\'' + generate_string_dataset(row['lan2_Wiki']) + '\\'' + ' \\'languages/' + row['lan2_Wiki'] + '/dataset_eval.txt\\'' ' \\'4350\\'\\n')\n",
    "        #runs language pair\n",
    "        file.write('python3 SimpleTransformers.py \\'' + generate_string_dataset(row['lan1_Wiki']) + '\\'' + ' \\'languages/' + str(d) + '/dataset_eval.txt\\' ' + '\\'results/output_' + str(d) + '/' + row['LAN_1'] + '_' + row['LAN_2'] + '\\'\\n')\n",
    "        file.write('\\n')\n",
    "        file.write('\\n')\n",
    "\n",
    "    file.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution : ___main___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     DIST LAN_1 lan1_MembES lan1_Wiki LAN_2 lan2_MembES lan2_Wiki\n",
      "0     0.0    Fr        True        fr    Fr        True        fr\n",
      "448   0.0    Cz        True        cs    Cz        True        cs\n",
      "416   0.0   Hng        True        hu   Hng        True        hu\n",
      "384   0.0   Swd        True        sv   Swd        True        sv\n",
      "383   0.0   Swd        True        sv   Nor        True        no\n",
      "\n",
      "0.000000     49\n",
      "1.000000     42\n",
      "1.414214     80\n",
      "1.732051    100\n",
      "2.000000    132\n",
      "2.236068    154\n",
      "2.449490    126\n",
      "2.645751    102\n",
      "2.828427     72\n",
      "3.000000     58\n",
      "3.162278     32\n",
      "3.316625     12\n",
      "3.464102      2\n",
      "Name: DIST, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_euclidean_dist = calculate_distances(lan_EuropeanSprachbund)\n",
    "df_euclidean_dist\n",
    "print(df_euclidean_dist.head())\n",
    "\n",
    "print()\n",
    "\n",
    "number_of_experiments = df_euclidean_dist['DIST'].value_counts().sort_index()\n",
    "print(number_of_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example \n",
    "d1 = 0.0\n",
    "\n",
    "generate_script_file(df_euclidean_dist, d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
