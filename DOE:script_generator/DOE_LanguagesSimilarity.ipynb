{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOE: Language Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description:\n",
    "\n",
    "1) Read in language vectors and their dataset abbreviation\n",
    "\n",
    "2) Calculate Euclidean distances\n",
    "\n",
    "3) Filter for closest and not closest language pairs with path to dataset files\n",
    "\n",
    "4) Generate script file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.spatial import distance_matrix\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Read in language vectors and corresponding Wikiann data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len before removing lan < 100T lines: 38\n",
      "len lan < 100T lines: 7\n",
      "len after removing lan < 100T lines: 31\n"
     ]
    }
   ],
   "source": [
    "lan_EuropeanSprachbund_raw = pd.read_csv('../overview_EuropeanSprachbund.csv', sep=';')\n",
    "\n",
    "# remove languages with dataset < 100T lines\n",
    "print('len before removing lan < 100T lines: ' + str(len(lan_EuropeanSprachbund_raw)))\n",
    "\n",
    "languages_with_less_100Tlines = [\"Ice\",\"Ir\",\"Gae\",\"Kom\",\"Mlt\",\"Srd\",\"Udm\"]\n",
    "print('len lan < 100T lines: ' + str(len(languages_with_less_100Tlines)))\n",
    "indices = [i for i in range(0,len(lan_EuropeanSprachbund_raw.index)) if lan_EuropeanSprachbund_raw['language'][i] not in languages_with_less_100Tlines]\n",
    "lan_EuropeanSprachbund = lan_EuropeanSprachbund_raw.iloc[indices].reset_index()\n",
    "\n",
    "print('len after removing lan < 100T lines: ' + str(len(lan_EuropeanSprachbund)))\n",
    "\n",
    "#lan_EuropeanSprachbund[lan_EuropeanSprachbund.columns[6:18]][['5','8','10','12']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrrrr}\n",
      "\\toprule\n",
      "language &  1 &  2 &  3 &  4 &  5 &  6 &  7 &  8 &  9 &  10 &  11 &  12 \\\\\n",
      "\\midrule\n",
      "      Fr &  1 &  1 &  1 &  1 &  1 &  1 &  1 &  1 &  1 &   1 &   1 &   1 \\\\\n",
      "     Grm &  1 &  1 &  1 &  1 &  1 &  1 &  1 &  1 &  0 &   1 &   1 &   1 \\\\\n",
      "     Dut &  1 &  1 &  1 &  1 &  1 &  0 &  1 &  1 &  1 &   1 &   1 &   0 \\\\\n",
      "     Eng &  1 &  1 &  1 &  1 &  1 &  1 &  0 &  1 &  1 &   1 &   1 &   0 \\\\\n",
      "     Grk &  1 &  1 &  1 &  1 &  1 &  1 &  1 &  0 &  1 &   1 &   0 &   1 \\\\\n",
      "     Spn &  1 &  1 &  1 &  1 &  1 &  0 &  1 &  1 &  0 &   1 &   0 &   1 \\\\\n",
      "     Prt &  1 &  1 &  1 &  1 &  1 &  0 &  1 &  1 &  0 &   1 &   0 &   1 \\\\\n",
      "      It &  1 &  1 &  1 &  1 &  1 &  0 &  1 &  1 &  0 &   1 &   0 &   1 \\\\\n",
      "     Alb &  1 &  1 &  1 &  0 &  1 &  0 &  1 &  1 &  1 &   1 &   0 &   1 \\\\\n",
      "     Srd &  1 &  1 &  1 &  0 &  1 &  0 &  1 &  1 &  0 &   1 &   0 &   1 \\\\\n",
      "     Rom &  1 &  1 &  1 &  0 &  1 &  1 &  1 &  0 &  0 &   1 &   0 &   1 \\\\\n",
      "     Rus &  0 &  1 &  0 &  0 &  1 &  1 &  1 &  0 &  1 &   1 &   1 &   1 \\\\\n",
      "     Nor &  1 &  1 &  1 &  1 &  1 &  0 &  0 &  1 &  0 &   0 &   0 &   1 \\\\\n",
      "     Swd &  1 &  1 &  1 &  1 &  1 &  0 &  0 &  1 &  0 &   0 &   0 &   1 \\\\\n",
      "     Hng &  1 &  1 &  0 &  1 &  1 &  0 &  1 &  0 &  1 &   1 &   0 &   0 \\\\\n",
      "      Cz &  0 &  1 &  1 &  1 &  1 &  0 &  1 &  0 &  0 &   1 &   0 &   1 \\\\\n",
      "     Ice &  0 &  1 &  1 &  0 &  1 &  0 &  0 &  1 &  0 &   0 &   1 &   1 \\\\\n",
      "     Ltv &  0 &  1 &  0 &  0 &  1 &  0 &  1 &  0 &  1 &   1 &   0 &   1 \\\\\n",
      "     Lit &  0 &  1 &  0 &  0 &  1 &  1 &  1 &  0 &  0 &   1 &   0 &   1 \\\\\n",
      "     SCr &  0 &  1 &  0 &  1 &  1 &  0 &  1 &  0 &  0 &   1 &   0 &   1 \\\\\n",
      "     Blg &  0 &  1 &  0 &  1 &  1 &  0 &  1 &  0 &  0 &   1 &   0 &   1 \\\\\n",
      "     Pol &  0 &  1 &  0 &  0 &  1 &  0 &  1 &  0 &  0 &   1 &   0 &   1 \\\\\n",
      "     Sln &  0 &  1 &  0 &  0 &  1 &  0 &  1 &  0 &  0 &   1 &   0 &   1 \\\\\n",
      "     Ukr &  0 &  1 &  0 &  0 &  1 &  0 &  1 &  0 &  0 &   1 &   0 &   1 \\\\\n",
      "     Bsq &  0 &  0 &  0 &  1 &  0 &  0 &  1 &  0 &  1 &   0 &   0 &   1 \\\\\n",
      "     Brt &  1 &  0 &  0 &  1 &  1 &  0 &  0 &  0 &  0 &   0 &   0 &   0 \\\\\n",
      "     Fin &  0 &  1 &  0 &  0 &  0 &  0 &  1 &  0 &  1 &   0 &   0 &   0 \\\\\n",
      "     Mlt &  0 &  0 &  0 &  1 &  1 &  0 &  1 &  0 &  0 &   0 &   0 &   0 \\\\\n",
      "     Grg &  0 &  1 &  0 &  0 &  0 &  0 &  0 &  1 &  0 &   0 &   0 &   0 \\\\\n",
      "      Ir &  0 &  0 &  0 &  0 &  1 &  0 &  0 &  0 &  0 &   0 &   0 &   0 \\\\\n",
      "     Wel &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &   0 &   1 &   0 \\\\\n",
      "     Est &  0 &  1 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &   0 &   0 &   0 \\\\\n",
      "     Arm &  0 &  1 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &   0 &   0 &   0 \\\\\n",
      "     Trk &  0 &  0 &  0 &  1 &  0 &  0 &  0 &  0 &  0 &   0 &   0 &   0 \\\\\n",
      "     Gae &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  1 &   0 &   0 &   0 \\\\\n",
      "     Kom &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &   0 &   0 &   0 \\\\\n",
      "     Udm &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &   0 &   0 &   0 \\\\\n",
      "     Tat &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &   0 &   0 &   0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lan_EuropeanSprachbund_raw[['language','1','2','3','4','5','6','7','8','9','10','11','12']].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Calculate Euclidean distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates all the distances between the language vectors\n",
    "def calculate_distances(df):\n",
    "\n",
    "    matrix = df[df.columns[6:18]].to_numpy()\n",
    "    #two-dimensional array of distances between all language pairs\n",
    "    distances = distance_matrix(matrix,matrix) \n",
    "\n",
    "    df_new = pd.DataFrame(columns=['DIST','LAN_1','lan1_MembES','lan1_Wiki','LAN_2','lan2_MembES','lan2_Wiki'])\n",
    "    list_ofSeries = []\n",
    "\n",
    "    for i in range(0,len(distances)): \n",
    "        for j in range(0,len(distances[i])):\n",
    "            \n",
    "            list_ofSeries += [pd.Series([distances[i][j],lan_EuropeanSprachbund['language'][i], lan_EuropeanSprachbund['Member in European Sprachbund'][i],\n",
    "                                         lan_EuropeanSprachbund['Wikiann'][i],lan_EuropeanSprachbund['language'][j],\n",
    "                                         lan_EuropeanSprachbund['Member in European Sprachbund'][j], lan_EuropeanSprachbund['Wikiann'][j]], index=df_new.columns)]\n",
    "    Df_distances = df_new.append(list_ofSeries, ignore_index=True).sort_values(by=['DIST'])\n",
    "    \n",
    "    return Df_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Filter for closest and not closest language pairs with path to dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters dataframe for given distance\n",
    "def dist_filter(df, dist):\n",
    "    \n",
    "    # filter out monolingual results\n",
    "    df = df[df['LAN_1'] != df['LAN_2']]\n",
    "    \n",
    "    #reduce the number of experiments to 40\n",
    "    return df[df['DIST'] == dist][:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets needed for the experiment - just done once\n",
    "# returns Wikiann dataset abbreviations\n",
    "\n",
    "def used_datasets(df):\n",
    "    \n",
    "    LAN1 = df['lan1_Wiki'].unique()\n",
    "    LAN2 = df['lan2_Wiki'].unique()\n",
    "    \n",
    "    datasets= list(set(LAN1) | set(LAN2))\n",
    "    \n",
    "    return datasets    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Generate script file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate path string for language\n",
    "def generate_string_dataset(wikiann_abre):\n",
    "    return '\"languages/' + wikiann_abre + '/dataset_$j.txt\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of datasets with paths on server\n",
    "def used_datasets_paths(datasets):\n",
    "    \n",
    "    datasets_paths = []\n",
    "    \n",
    "    for i in range(0,len(datasets)):\n",
    "        datasets_paths + [generate_string_dataset(datasets[i])]\n",
    "    \n",
    "    return datasets_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_script_file(df_raw, d):\n",
    "    \n",
    "    df = dist_filter(df_raw, d)\n",
    "    number_of_experiments = df['DIST'].value_counts().sort_index()[d]\n",
    "    \n",
    "    used_datasets_with_paths = used_datasets_paths(used_datasets(df))\n",
    "    \n",
    "    file = open('run_transformers_' + str(round(d,2)) + '.sh','w')\n",
    "    \n",
    "    file.write('# Experiments for distance: ' + str(d) + '\\n')\n",
    "    file.write('\\n')\n",
    "    file.write('# Experiment design: \\n')\n",
    "    file.write('# (Finetuned language, Evaluated language) - Split(80/20)\\n')\n",
    "    file.write('# Number of language pairs: ' + str(number_of_experiments) + '\\n')\n",
    "    file.write('\\n')\n",
    "    file.write('# @author: jopo, phze \\n')\n",
    "    file.write('# @date: ' + str(datetime.datetime.now()) + '\\n')\n",
    "    file.write('\\n')\n",
    "    file.write('\\n')\n",
    "    file.write('#!/bin/bash\\n')\n",
    "    file.write('\\n')\n",
    "    file.write('# Cross-validation on different subset-reduction of datasets (preprocessed):\\n')\n",
    "    file.write(' for j in {1..4}\\n')\n",
    "    file.write(' do\\n')\n",
    "    file.write('\\n')\n",
    "    file.write('\\n')\n",
    "    \n",
    "    # running experiments for language pairs\n",
    "    file.write('# Experiments:\\n')\n",
    "    for index, row in df.iterrows():\n",
    "        file.write ('# ' + str(index) + ' - Language pair: ' + '(' + row.LAN_1 + ',' + row.LAN_2 + ')\\n')          \n",
    "        \n",
    "        #creates test-set '4350' 20% -> dataset is of '17003' sentences = 80% training set\n",
    "        \n",
    "        # python3 take_sentences.py wikiann-en.bio train-en.bio test-en.bio 0 \n",
    "        file.write('    python3 preprocessing/take_sentences.py ' + generate_string_dataset(row.lan1_Wiki) + ' \"languages/' + row.lan1_Wiki + '/dataset_train.txt\"' + ' \"languages/' + row.lan1_Wiki + '/dataset_eval.txt\"' ' \\\"0\"\\n')\n",
    "        file.write('    python3 preprocessing/take_sentences.py ' + generate_string_dataset(row.lan2_Wiki) + ' \"languages/' + row.lan2_Wiki + '/dataset_train.txt\"' + ' \"languages/' + row.lan2_Wiki + '/dataset_eval.txt\"' ' \\\"0\"\\n')\n",
    "        #runs language pair\n",
    "        file.write('    python3 SimpleTransformers.py ' + '\"languages/' + row.lan1_Wiki + '/dataset_train.txt\"' + ' \"languages/' + row.lan2_Wiki + '/dataset_eval.txt\" ' + '\"results_filt/results_' + str(round(d,2)) + '/output_' + str(round(d,2)) + '/' + row.LAN_1 + '_' + row.LAN_2 + '_' + '$j' + '\"\\n')\n",
    "        file.write('\\n')\n",
    "    \n",
    "    file.write(' done')\n",
    "\n",
    "    file.closed\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution : ___main___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000     49\n",
      "1.000000     42\n",
      "1.414214     80\n",
      "1.732051    100\n",
      "2.000000    132\n",
      "2.236068    154\n",
      "2.449490    126\n",
      "2.645751    102\n",
      "2.828427     72\n",
      "3.000000     58\n",
      "3.162278     32\n",
      "3.316625     12\n",
      "3.464102      2\n",
      "Name: DIST, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_euclidean_dist = calculate_distances(lan_EuropeanSprachbund)\n",
    "\n",
    "number_of_experiments = df_euclidean_dist['DIST'].value_counts().sort_index()\n",
    "print(number_of_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7320508075688772"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_experiments.index[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d7 2.65 :7.000000000000001\n"
     ]
    }
   ],
   "source": [
    "# Number of features not in common:\n",
    "#d1 = number_of_experiments.index[5]\n",
    "#print('d1 ' + str(round(d1,2)) + ' :' + str(d1 ** 2))\n",
    "\n",
    "#d2 = number_of_experiments.index[0]\n",
    "#print('d2 ' + str(round(d2,2)) + ' :' + str(d2 ** 2))\n",
    "\n",
    "#d3 = number_of_experiments.index[2]\n",
    "#print('d3 ' + str(round(d3,2)) + ' :' + str(d3 ** 2))\n",
    "\n",
    "#d4 = number_of_experiments.index[-3]\n",
    "#print('d4 ' + str(round(d4,2)) + ' :' + str(d4 ** 2))\n",
    "\n",
    "#d5 = number_of_experiments.index[-2]\n",
    "#print('d5 ' + str(round(d5,2)) + ' :' + str(d5 ** 2))\n",
    "\n",
    "#d6 = number_of_experiments.index[-1]\n",
    "#print('d6 ' + str(round(d6,2)) + ' :' + str(d6 ** 2))\n",
    "\n",
    "d7 = number_of_experiments.index[7]\n",
    "print('d7 ' + str(round(d7,2)) + ' :' + str(d7 ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6457513110645907\n"
     ]
    }
   ],
   "source": [
    "# design points of experiments: distances{d1 .. d5}\n",
    "\n",
    "#print(d1)\n",
    "#df_d1 = generate_script_file(df_euclidean_dist, d1)\n",
    "\n",
    "#print(d2)\n",
    "#df_d2 = generate_script_file(df_euclidean_dist, d2)\n",
    "\n",
    "#print(d3)\n",
    "#df_d3 = generate_script_file(df_euclidean_dist, d3)\n",
    "\n",
    "#print(d4)\n",
    "#generate_script_file(df_euclidean_dist, d4)\n",
    "\n",
    "#print(d5)\n",
    "#generate_script_file(df_euclidean_dist, d5)\n",
    "\n",
    "#print(d6)\n",
    "#df_d6 = generate_script_file(df_euclidean_dist, d6)\n",
    "\n",
    "print(d7)\n",
    "df_d7 = generate_script_file(df_euclidean_dist, d7)\n",
    "\n",
    "#pd.concat[[df_d2,df_d3,df_d4]].to_csv('Designpoints_LanSim_filt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
